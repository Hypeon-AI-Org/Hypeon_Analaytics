# =============================================================================
# HypeOn Analytics V1 - Environment example
# Copy to .env and fill in values. Do not commit .env.
# =============================================================================

# ----- GCP / BigQuery (backend, scripts, agents) -----
# BigQuery auth (not used for Gemini; Copilot uses AI Studio API key below).
# Local: use gcloud auth (no key file). Run: gcloud auth application-default login
# Then leave GOOGLE_APPLICATION_CREDENTIALS unset; BigQuery will use ADC.
# Cloud Run / CI: set GOOGLE_APPLICATION_CREDENTIALS to a service account key path, or use workload identity.
# GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account-key.json

# Application DB project (analytics_insights, marketing_performance_daily, marts, etc.)
BQ_PROJECT=your-app-gcp-project-id
# Source/input project (Ads + GA4 raw data). Omit or set same as BQ_PROJECT for single-project setup
BQ_SOURCE_PROJECT=your-source-gcp-project-id
ANALYTICS_DATASET=analytics
ADS_DATASET=146568
GA4_DATASET=analytics_444259275
# BigQuery job/dataset region for GA4 and final table (e.g. europe-north2)
# BQ_LOCATION=europe-north2
# Optional: serve insights from local JSON path
# INSIGHTS_JSON_PATH=/path/to/insights.json
# Ads source is in EU; job runs in EU, then staging is copied to BQ_LOCATION
# BQ_LOCATION_ADS=EU
# Dataset in BQ_PROJECT (EU) for Ads staging; created automatically if missing
# STAGING_EU_DATASET=staging_eu

# ----- API (backend) -----
API_KEY=your-api-key-for-x-api-key-header
ENV=dev
CORS_ORIGINS=http://localhost:3000,http://localhost:5173
JWT_SECRET=

# ----- Analytics cache (Google Cloud Memorystore for Redis recommended) -----
# Set REDIS_URL to use Redis; omit for in-memory fallback (cache lost on restart).
# Memorystore for Redis: use the instance internal IP (same VPC as Cloud Run / GKE).
#   Create in Console: Memorystore > Redis > Create; then copy Primary endpoint IP.
#   Format: redis://<PRIMARY_IP>:6379/0
#   If AUTH is enabled: redis://:YOUR_AUTH_STRING@<PRIMARY_IP>:6379/0
# Local dev: redis://localhost:6379/0
# REDIS_URL=redis://10.0.0.3:6379/0

# ----- Copilot: LLM backend (Claude preferred; Gemini fallback) -----
# If ANTHROPIC_API_KEY is set, Copilot uses Claude. Otherwise Gemini (if configured).
# Optional: COPILOT_MAX_OUTPUT_TOKENS (default 2048) applies to both.

# Claude (Anthropic) — preferred. Get key at https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key
# Optional: model (default claude-3-5-sonnet-20241022)
# CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Gemini (Google) — used when ANTHROPIC_API_KEY is not set. Get key at https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your-gemini-api-key
# Optional: model name (default gemini-2.0-flash)
# GEMINI_MODEL=gemini-2.0-flash
# Option B: Vertex AI (GCP) instead of AI Studio — uses ADC / GOOGLE_APPLICATION_CREDENTIALS
# GOOGLE_GENAI_USE_VERTEXAI=true
# GOOGLE_CLOUD_PROJECT=your-gcp-project
# GOOGLE_CLOUD_LOCATION=us-central1

# ----- Optional: logging -----
# LOG_LEVEL=INFO (default). Every API request is logged: METHOD path ... then -> status | duration.
# Use INFO for readable live logs; DEBUG floods with google/urllib3. From backend/: .\run_backend.ps1
# UVICORN_ACCESS_LOG=0 to hide uvicorn's own access lines; app request logging is independent (see above).
# AUDIT_STDOUT=1

# ----- Airflow / Cloud Composer (optional: cache refresh DAG) -----
# HYPEON_REPO_PATH=/home/airflow/gcs/dags/hypeon-analytics
# PYTHON=python
# REFRESH_CACHE_URL=https://your-api.example.com  # DAG calls this to refresh cache

# =============================================================================
# Frontend (Vite) - use in frontend/.env or frontend/.env.local
# =============================================================================
# VITE_API_BASE=
# VITE_API_KEY=your-api-key-for-x-api-key-header
